expected_cluster_size: "{{ groups['workers'] | length }}"

# This variable dynamically determines the IP address for Nomad to advertise.
# It prefers the first non-loopback IPv6 address if available,
# otherwise it falls back to the default IPv4 address.
advertise_ip: "{{ (ansible_all_ipv6_addresses | reject('equalto', '::1') | list | first) |
default(ansible_default_ipv4.address) }}"

nomad_models_dir: "/opt/nomad/models"

# Pipecat application settings
prima_api_service_name: "prima-api-main"
use_summarizer: "false"
stt_service: "faster-whisper"
pipecat_app_dir: "/opt/pipecatapp"
pipecat_log_file: "/tmp/pipecat.log"
# ... existing variables ...
active_stt_provider: "faster-whisper"
active_stt_model_name: "tiny.en"

# 1. Generate a new, secure key
#python -c "import secrets; print(secrets.token_hex(32))"
# Example output: a1b2c3d4e5f6...

# 2. Hash the key (replace <your_key> with the output from the first command)
#echo -n "<your_key>" | sha256sum
# Example output: <hashed_key>  -
pipecat_api_keys: ""

# ... existing variables ...

# Configuration for external, third-party LLM experts.
# This is a dictionary that will be converted to a JSON string.
external_experts_config:
  openai_gpt4:
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    model: "gpt-4-turbo" # Specify the exact model name

# -- Storing Secrets with Ansible Vault --
# This is the plain text version. In a real environment, you should
# encrypt this variable. First, create an encrypted file:
#   ansible-vault create group_vars/secrets.yaml
# Then, add the following to that file:
#   openai_api_key: "sk-..."
# Your main playbook should then include:
#   vars_files:
#     - group_vars/secrets.yaml
# For now, we'll place it here for simplicity.
openai_api_key: "sk-your_openai_api_key_here"

# Model definitions have been moved to group_vars/models.yaml
     #  env {
        # This should match the service name of the main prima-expert job
        # PRIMA_API_SERVICE_NAME = "prima-api-main"
        # Set to "true" to enable the summarizer tool
        # USE_SUMMARIZER = "false"
        # The vision and embedding models are now hardcoded in the application
        # to load from the unified /opt/nomad/models directory.
        # STT_SERVICE = "faster-whisper"
        # STT_MODEL_PATH = "/opt/nomad/models/stt/{{ active_stt_provider }}/{{ active_stt_model_name }}"
        # A comma-separated list of SHA-256 hashed API keys.
        # Generate a key with: python -c "import secrets; print(secrets.token_hex(32))"
        # Then hash it with: echo -n "<your_key>" | sha256sum
        # PIECAT_API_KEYS = ""

        # Configuration for external, third-party LLM experts.
        # This is a JSON string defining a dictionary where each key is the expert's name.
        # - "base_url": The API endpoint for the expert.
        # - "api_key_env": The name of the environment variable that holds the API key.
        #
        # Example:
        # EXTERNAL_EXPERTS_CONFIG = <<EOF
        # {
        #   "openai_gpt4": {
        #     "base_url": "https://api.openai.com/v1",
        #     "api_key_env": "OPENAI_API_KEY"
        #   }
        # }
        # EOF
        # OPENAI_API_KEY = "sk-..."

     #  }

# --- PXE Boot Configuration ---

# Select the operating system for the PXE server.
# Options: "debian" or "nixos". Defaults to "debian" for backward compatibility.
pxe_os: "debian"

# -- NixOS PXE Server Variables --
# These variables are only used if pxe_os is set to "nixos".

# The network interface on the PXE server for DHCP/TFTP/HTTP services.
pxe_interface: "enp0s8"

# The public SSH key to embed in the NixOS clients for root access.
# Replace this with your actual public key.
pxe_client_ssh_key: "ssh-rsa AAAA..."

# DHCP network configuration for NixOS clients.
pxe_subnet: "192.168.1.0"
pxe_netmask: "255.255.255.0"
pxe_range_start: "192.168.1.200"
pxe_range_end: "192.168.1.250"
pxe_router: "192.168.1.1"
