# Default configuration for the distributed-llama service

# User to run the service as
# Ensure this user exists and has permissions to the INSTALL_DIR, PID_DIR_BASE, and LOG_DIR_BASE
SERVICE_USER="llamauser"

# Installation directory where distributed-llama-repo is located
# Example: /opt/distributed-llama (if you ran setup_distributed_llama.sh there)
INSTALL_DIR="/opt/distributed-llama"

# Path to the model file (REQUIRED - must be set by the user)
# Example: "/opt/distributed-llama/distributed-llama-repo/models/your_model.gguf"
MODEL_PATH=""

# Path to the tokenizer file (REQUIRED - must be set by the user)
# Example: "/opt/distributed-llama/distributed-llama-repo/tokenizers/your_tokenizer.model"
TOKENIZER_PATH=""

# Host for the API server (e.g., 0.0.0.0 to listen on all interfaces)
API_HOST="0.0.0.0"

# Port for the API server
API_PORT="8080"

# Additional command-line options for the dllama-api executable
# Example: "--nthreads 4"
DAEMON_OPTS=""

# Name of the repository directory inside INSTALL_DIR
# (Typically "distributed-llama-repo" if using the setup script)
REPO_DIR_NAME="distributed-llama-repo"

# Name of the main daemon executable
# (Typically "dllama-api" if using the setup script)
DAEMON_EXECUTABLE_NAME="dllama-api"
