- name: Install build and runtime dependencies for llama.cpp
  ansible.builtin.apt:
    name:
      - build-essential
      - git
      - cmake
      - libssl-dev
      - libcurl4-openssl-dev
      - libstdc++6
    state: present
  become: yes

- name: Build and install llama.cpp
  become: yes
  block:
    - name: Create build directory for llama.cpp
      ansible.builtin.file:
        path: /opt/llama.cpp-build
        state: directory
        mode: '0755'

    - name: Clone llama.cpp repository
      ansible.builtin.git:
        repo: 'https://github.com/ggml-org/llama.cpp.git'
        dest: /opt/llama.cpp-build
        version: master
        update: yes
      register: llama_git_result

    - name: Build and install from source if updated
      block:
        - name: Configure llama.cpp build with RPC and shared libraries
          ansible.builtin.command:
            cmd: cmake -B build -DGGML_RPC=ON -DLLAMA_SERVER_SSL=ON -DBUILD_SHARED_LIBS=ON
          args:
            chdir: /opt/llama.cpp-build
            creates: /opt/llama.cpp-build/build/Makefile

        - name: Build llama.cpp
          ansible.builtin.command:
            cmd: cmake --build build --config Release -j 2
          args:
            chdir: /opt/llama.cpp-build
            creates: /opt/llama.cpp-build/build/bin/llama-server

        - name: Find all compiled binaries and libraries
          ansible.builtin.find:
            paths: "/opt/llama.cpp-build/build/bin"
            patterns: '*'
            file_type: any
          register: llama_artifacts

        - name: Install all compiled binaries to /usr/local/bin
          ansible.builtin.copy:
            src: "{{ item.path }}"
            dest: "/usr/local/bin/{{ item.path | basename }}"
            mode: '0755'
            remote_src: yes
          loop: "{{ llama_artifacts.files | rejectattr('path', 'match', '.*\\.so$') | list }}"

        - name: Install shared libraries to /usr/local/lib
          ansible.builtin.copy:
            src: "{{ item.path }}"
            dest: "/usr/local/lib/{{ item.path | basename }}"
            mode: '0755'
            remote_src: yes
          loop: "{{ llama_artifacts.files | selectattr('path', 'match', '.*\\.so$') | list }}"
          notify: update ld cache

        - name: Clean up llama.cpp build directory
          ansible.builtin.file:
            path: /opt/llama.cpp-build
            state: absent
      when: llama_git_result.changed


- name: Create Nomad jobs directory
  ansible.builtin.file:
    path: /opt/nomad/jobs
    state: directory
    mode: '0755'
  become: yes

- name: Stop and purge any existing expert jobs to ensure idempotency
  ansible.builtin.command:
    cmd: nomad job stop -purge "expert-{{ item }}"
  loop: "{{ experts }}"
  register: expert_job_stop_status
  changed_when: "'Running' in expert_job_stop_status.stdout"
  failed_when: false # Don't fail if the job doesn't exist
  environment:
    NOMAD_ADDR: "http://{{ ansible_default_ipv4.address }}:4646"
    
- name: Install build and runtime dependencies for llama.cpp
  ansible.builtin.apt:
    name:
      - build-essential
      - git
      - cmake
      - libssl-dev
      - libcurl4-openssl-dev
      - libstdc++6
    state: present
  become: yes

- name: Build and install llama.cpp
  become: yes
  block:
    - name: Create build directory for llama.cpp
      ansible.builtin.file:
        path: /opt/llama.cpp-build
        state: directory
        mode: '0755'

    - name: Clone llama.cpp repository
      ansible.builtin.git:
        repo: 'https://github.com/ggml-org/llama.cpp.git'
        dest: /opt/llama.cpp-build
        version: master
        update: yes
      register: llama_git_result

    - name: Build and install from source if updated
      block:
        - name: Configure llama.cpp build with RPC and shared libraries
          ansible.builtin.command:
            cmd: cmake -B build -DGGML_RPC=ON -DLLAMA_SERVER_SSL=ON -DBUILD_SHARED_LIBS=ON
          args:
            chdir: /opt/llama.cpp-build
            creates: /opt/llama.cpp-build/build/Makefile

        - name: Build llama.cpp
          ansible.builtin.command:
            cmd: cmake --build build --config Release -j 2
          args:
            chdir: /opt/llama.cpp-build
            creates: /opt/llama.cpp-build/build/bin/llama-server

        - name: Find all compiled binaries and libraries
          ansible.builtin.find:
            paths: "/opt/llama.cpp-build/build/bin"
            patterns: '*'
            file_type: any
          register: llama_artifacts

        - name: Install all compiled binaries to /usr/local/bin
          ansible.builtin.copy:
            src: "{{ item.path }}"
            dest: "/usr/local/bin/{{ item.path | basename }}"
            mode: '0755'
            remote_src: yes
          loop: "{{ llama_artifacts.files | rejectattr('path', 'match', '.*\\.so$') | list }}"

        - name: Install shared libraries to /usr/local/lib
          ansible.builtin.copy:
            src: "{{ item.path }}"
            dest: "/usr/local/lib/{{ item.path | basename }}"
            mode: '0755'
            remote_src: yes
          loop: "{{ llama_artifacts.files | selectattr('path', 'match', '.*\\.so$') | list }}"
          notify: update ld cache

        - name: Clean up llama.cpp build directory
          ansible.builtin.file:
            path: /opt/llama.cpp-build
            state: absent
      when: llama_git_result.changed

- name: Prepare for model benchmarking
  become: yes
  block:
    - name: Clear previous benchmark results
      ansible.builtin.file:
        path: /var/log/llama_benchmarks.json
        state: absent

    - name: Create empty benchmark results file
      ansible.builtin.file:
        path: /var/log/llama_benchmarks.json
        state: touch
        mode: '0644'
        owner: "{{ target_user }}"
        group: "{{ target_user }}"

- name: Flatten model list for benchmarking
  ansible.builtin.set_fact:
    all_models_to_benchmark: "{{ expert_models.values() | sum(start=[]) | unique(attribute='filename') }}"

- name: Benchmark each available model and record results
  loop: "{{ all_models_to_benchmark }}"
  loop_control:
    loop_var: model_item
  block:
    - name: "Run llama-bench for {{ model_item.filename }}"
      ansible.builtin.command:
        cmd: "llama-bench -m /opt/nomad/models/llm/{{ model_item.filename }} -p 32 -n 32"
      register: benchmark_result
      ignore_errors: true
      changed_when: false
      become: yes
      become_user: "{{ target_user }}"

    - name: "Append benchmark result for {{ model_item.filename }}"
      ansible.builtin.lineinfile:
        path: /var/log/llama_benchmarks.jsonl
        line: "{{ {'model': model_item.filename, 'status': 'success' if benchmark_result.rc == 0 else 'fail', 'tokens_per_second': (benchmark_result.stdout | regex_search('total tokens/s.*= *([0-9]+\\\\.[0-9]+)', '\\\\1') | first | float) if benchmark_result.rc == 0 else 0} | to_json }}"
      become: yes


