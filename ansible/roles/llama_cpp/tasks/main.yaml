---
- name: Install build dependencies for llama.cpp
  ansible.builtin.apt:
    name:
      - build-essential
      - git
      - cmake
    state: present
  become: yes

- name: Create build directory for llama.cpp
  ansible.builtin.file:
    path: /opt/llama.cpp-build
    state: directory
    mode: '0755'
  become: yes

- name: Clone llama.cpp repository
  ansible.builtin.git:
    repo: 'https://github.com/ggml-org/llama.cpp.git'
    dest: /opt/llama.cpp-build
    version: master
    update: yes
    force: yes
  become: yes

- name: Configure llama.cpp build
  ansible.builtin.command:
    cmd: cmake -B build -DGGML_RPC=ON -DLLAMA_SERVER_SSL=ON
  args:
    chdir: /opt/llama.cpp-build
    creates: /opt/llama.cpp-build/build/Makefile
  become: yes

- name: Build llama.cpp
  ansible.builtin.command:
    cmd: cmake --build build --config Release -j
  args:
    chdir: /opt/llama.cpp-build
    creates: /opt/llama.cpp-build/build/bin/llama-server
  become: yes

- name: Install llama-server binary
  ansible.builtin.copy:
    src: /opt/llama.cpp-build/build/bin/llama-server
    dest: /usr/local/bin/llama-server
    mode: '0755'
    remote_src: yes
  become: yes

- name: Verify llama-server binary installation
  ansible.builtin.stat:
    path: /usr/local/bin/llama-server
  register: llama_server_stat

- name: Fail if llama-server is not installed or not executable
  ansible.builtin.fail:
    msg: "llama-server was not found at /usr/local/bin/llama-server or is not executable."
  when: not llama_server_stat.stat.exists or not llama_server_stat.stat.executable

- name: Clean up llama.cpp build directory
  ansible.builtin.file:
    path: /opt/llama.cpp-build
    state: absent
  become: yes

- name: Create Nomad jobs directory
  ansible.builtin.file:
    path: /opt/nomad/jobs
    state: directory
    mode: '0755'
  become: yes

- name: Create models directory on host
  ansible.builtin.file:
    path: /models
    state: directory
    mode: '0755'
  become: yes

- name: Download Llama 3 model
  ansible.builtin.get_url:
    url: "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
    dest: "/models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
    mode: '0644'
  become: yes

- name: Copy llama.cpp RPC Nomad job file
  ansible.builtin.template:
    src: ../../jobs/llamacpp-rpc.nomad
    dest: /opt/nomad/jobs/llamacpp-rpc.nomad
  when: "'controller_nodes' in group_names"
  vars:
    meta:
      NAMESPACE: "default"
      JOB_NAME: "llama-3-8b-instruct"
      API_SERVICE_NAME: "llama-api-llama-3-8b-instruct"
      RPC_SERVICE_NAME: "llama-rpc-worker-llama-3-8b-instruct"
      MODEL_PATH: "/models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
      WORKER_COUNT: 2

- name: Copy benchmark Nomad job file
  ansible.builtin.template:
    src: ../../jobs/benchmark.nomad
    dest: /opt/nomad/jobs/benchmark.nomad
  when: "'controller_nodes' in group_names"
