- name: Install build and runtime dependencies for llama.cpp
  ansible.builtin.apt:
    name:
      - build-essential
      - git
      - cmake
      - libssl-dev
      - libcurl4-openssl-dev
      - libstdc++6
    state: present
  become: yes

- name: Get latest commit hash from llama.cpp remote repo
  ansible.builtin.command:
    cmd: "git ls-remote https://github.com/ggml-org/llama.cpp.git HEAD"
  register: git_ls_remote_result
  changed_when: false
  become: no

- name: Extract commit hash from git ls-remote output
  ansible.builtin.set_fact:
    latest_commit_hash: "{{ git_ls_remote_result.stdout.split()[0] }}"

- name: Check for existing llama.cpp version file
  ansible.builtin.stat:
    path: /usr/local/etc/llama-cpp.version
  register: version_file_stat
  become: yes

- name: Read installed llama.cpp version
  ansible.builtin.slurp:
    src: /usr/local/etc/llama-cpp.version
  when: version_file_stat.stat.exists
  register: installed_version_raw
  become: yes

- name: Set installed_version fact
  ansible.builtin.set_fact:
    installed_version: "{{ (installed_version_raw.content | b64decode | trim) if 'content' in installed_version_raw else 'none' }}"

- name: Decide if llama.cpp needs to be built
  ansible.builtin.set_fact:
    build_llama_cpp: "{{ installed_version != latest_commit_hash }}"

- name: Print versions for debugging
  ansible.builtin.debug:
    msg: "Installed version: {{ installed_version }}, Latest version: {{ latest_commit_hash }}. Build needed: {{ build_llama_cpp }}"

- name: Build and install llama.cpp
  become: yes
  block:
    - name: Create build directory for llama.cpp
      ansible.builtin.file:
        path: /opt/llama.cpp-build
        state: directory
        mode: '0755'

    - name: Clone llama.cpp repository
      ansible.builtin.git:
        repo: 'https://github.com/ggml-org/llama.cpp.git'
        dest: /opt/llama.cpp-build
        version: "{{ latest_commit_hash }}"
        update: yes

    - name: Configure llama.cpp build with RPC and shared libraries
      ansible.builtin.command:
        cmd: cmake -B build -DGGML_RPC=ON -DLLAMA_SERVER_SSL=ON -DBUILD_SHARED_LIBS=ON
      args:
        chdir: /opt/llama.cpp-build
        creates: /opt/llama.cpp-build/build/Makefile

    - name: Build llama.cpp
      ansible.builtin.command:
        cmd: cmake --build build --config Release -j 2
      args:
        chdir: /opt/llama.cpp-build
        creates: /opt/llama.cpp-build/build/bin/llama-server

    - name: Find all compiled binaries and libraries
      ansible.builtin.find:
        paths: "/opt/llama.cpp-build/build/bin"
        patterns: '*'
        file_type: any
      register: llama_artifacts

    - name: Install all compiled binaries to /usr/local/bin
      ansible.builtin.copy:
        src: "{{ item.path }}"
        dest: "/usr/local/bin/{{ item.path | basename }}"
        mode: '0755'
        remote_src: yes
      loop: "{{ llama_artifacts.files | rejectattr('path', 'match', '.*\\.so$') | list }}"

    - name: Install shared libraries to /usr/local/lib
      ansible.builtin.copy:
        src: "{{ item.path }}"
        dest: "/usr/local/lib/{{ item.path | basename }}"
        mode: '0755'
        remote_src: yes
      loop: "{{ llama_artifacts.files | selectattr('path', 'match', '.*\\.so$') | list }}"
      notify: update ld cache

#    - name: Flush handlers to update ldconfig
#      meta: flush_handlers

    - name: Stamp the installed version
      ansible.builtin.copy:
        content: "{{ latest_commit_hash }}"
        dest: /usr/local/etc/llama-cpp.version
        mode: '0644'

    - name: Clean up llama.cpp build directory
      ansible.builtin.file:
        path: /opt/llama.cpp-build
        state: absent
  when: build_llama_cpp

- name: Create Nomad jobs directory
  ansible.builtin.file:
    path: /opt/nomad/jobs
    state: directory
    mode: '0755'
  become: yes

- name: Flatten model list for RPC jobs
  ansible.builtin.set_fact:
    all_models_for_rpc: "{{ expert_models.values() | sum(start=[]) | unique(attribute='filename') }}"

- name: Stop and purge any existing llamacpp-rpc jobs if llama.cpp was rebuilt
  ansible.builtin.command:
    cmd: nomad job stop -purge "llamacpp-rpc-{{ item.filename | regex_replace('\\.gguf$', '') }}"
  loop: "{{ all_models_for_rpc }}"
  register: rpc_job_stop_status
  changed_when: "'Running' in rpc_job_stop_status.stdout"
  failed_when: false # Don't fail if the job doesn't exist
  environment:
    NOMAD_ADDR: "http://{{ ansible_default_ipv4.address }}:4646"
  when: build_llama_cpp



- name: Stop and purge any existing expert jobs if llama.cpp was rebuilt
  ansible.builtin.command:
    cmd: nomad job stop -purge "expert-{{ item }}"
  loop: "{{ experts }}"
  register: expert_job_stop_status
  changed_when: "'Running' in expert_job_stop_status.stdout"
  failed_when: false # Don't fail if the job doesn't exist
  environment:
    NOMAD_ADDR: "http://{{ ansible_default_ipv4.address }}:4646"
  when: build_llama_cpp

- name: Run model benchmarking if enabled
  block:
    - name: Prepare for model benchmarking
      become: yes
      block:
        - name: Clear previous benchmark results
          ansible.builtin.file:
            path: /var/log/llama_benchmarks.jsonl
            state: absent

        - name: Create empty benchmark results file
          ansible.builtin.file:
            path: /var/log/llama_benchmarks.jsonl
            state: touch
            mode: '0644'
            owner: "{{ target_user }}"
            group: "{{ target_user }}"

    - name: Flatten model list for benchmarking
      ansible.builtin.set_fact:
        all_models_to_benchmark: "{{ expert_models.values() | sum(start=[]) | unique(attribute='filename') }}"

    - name: Benchmark each available model and record results
      ansible.builtin.include_tasks: benchmark_single_model.yaml
      loop: "{{ all_models_to_benchmark }}"
      loop_control:
        loop_var: model_item
  when: run_benchmarks | default(false)

- name: Read benchmark log file
  ansible.builtin.slurp:
    src: /var/log/llama_benchmarks.jsonl
  register: benchmark_log_raw
  become: yes

- name: Parse benchmark JSONL content safely
  ansible.builtin.shell:
    cmd: |
      set -o pipefail
      output=""
      while IFS= read -r line; do
        if echo "$line" | jq -e . > /dev/null; then
          if [ -z "$output" ]; then
            output="[$line"
          else
            output="$output,$line"
          fi
        fi
      done < /var/log/llama_benchmarks.jsonl
      if [ -n "$output" ]; then
        output="$output]"
      else
        output="[]"
      fi
      echo "$output"
    executable: /bin/bash
  register: benchmark_parser_result
  changed_when: false
  become: yes

- name: Set benchmark_results fact from parsed content
  ansible.builtin.set_fact:
    benchmark_results: "{{ benchmark_parser_result.stdout | from_json }}"

- name: Template and run llamacpp-rpc nomad job for each model
  ansible.builtin.include_tasks: run_single_rpc_job.yaml
  loop: "{{ all_models_for_rpc }}" # Use the flattened list created earlier
  loop_control:
    loop_var: model_item # Use a distinct loop variable name
