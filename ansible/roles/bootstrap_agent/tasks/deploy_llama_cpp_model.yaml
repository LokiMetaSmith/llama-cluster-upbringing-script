- name: Render the Llama.cpp RPC Nomad job from template for bootstrap
  ansible.builtin.template:
    src: "{{ playbook_dir }}/ansible/jobs/llamacpp-rpc.nomad.j2"
    dest: "{{ nomad_jobs_dir }}/llamacpp-rpc.nomad"
    mode: '0644'
  vars:
    job_name: "llamacpp-rpc"
    service_name: "llamacpp-rpc-api"
    nomad_namespace: "default"
    model_list: "{{ expert_models['main'] }}"
    worker_count: 1
  register: job_file

- name: Check if llamacpp-rpc job is running
  ansible.builtin.command:
    cmd: "nomad job status llamacpp-rpc"
  register: job_status
  failed_when: false
  changed_when: false
  environment:
    NOMAD_ADDR: "http://{{ ansible_default_ipv4.address }}:4646"

- name: Deploy the main Llama.cpp RPC service to Nomad
  ansible.builtin.command:
    cmd: "nomad job run -detach {{ nomad_jobs_dir }}/llamacpp-rpc.nomad"
  register: llama_job_run
  changed_when: "'Eval ID' in llama_job_run.stdout"
  failed_when: llama_job_run.rc != 0
  environment:
    NOMAD_ADDR: "http://{{ ansible_default_ipv4.address }}:4646"
  when: job_file.changed or job_status.rc != 0

- name: Wait for the main expert service to be healthy in Consul
  ansible.builtin.uri:
    url: "http://127.0.0.1:8500/v1/health/service/llamacpp-rpc-api"
    return_content: yes
  register: expert_health
  until: >
    expert_health.json is defined and
    expert_health.json | map(attribute='Checks') | flatten | selectattr('Status', 'equalto', 'passing') | list | length > 0
  retries: 60
  delay: 10
  changed_when: false
