- name: Render the Llama.cpp RPC Nomad job from template for bootstrap
  ansible.builtin.template:
    src: "{{ playbook_dir }}/ansible/jobs/llamacpp-rpc.nomad"
    dest: "/tmp/llamacpp-rpc.nomad"
    mode: '0644'
  vars:
    job_name: "llamacpp-rpc"
    service_name: "llamacpp-rpc-api"
    namespace: "default"
    model_list: "{{ expert_models['main'] }}"
    worker_count: 1

- name: Deploy the main Llama.cpp RPC service to Nomad
  ansible.builtin.command:
    cmd: "nomad job run -detach /tmp/llamacpp-rpc.nomad"
  register: llama_job_run
  changed_when: "'Eval ID' in llama_job_run.stdout"
  failed_when: llama_job_run.rc != 0 and 'already running' not in llama_job_run.stderr
  environment:
    NOMAD_ADDR: "http://{{ ansible_default_ipv4.address }}:4646"

- name: Clean up temporary rendered job file
  ansible.builtin.file:
    path: "/tmp/llamacpp-rpc.nomad"
    state: absent

- name: Wait for the main expert service to be healthy in Consul
  ansible.builtin.uri:
    url: "http://127.0.0.1:8500/v1/health/service/llamacpp-rpc-api"
    return_content: yes
  register: expert_health
  until: >
    expert_health.json is defined and
    expert_health.json | map(attribute='Checks') | flatten | selectattr('Status', 'equalto', 'passing') | list | length > 0
  retries: 60
  delay: 10
  changed_when: false
