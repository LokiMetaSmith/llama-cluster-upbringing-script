- name: Render the llama.cpp Nomad job from template
  ansible.builtin.template:
    src: "{{ playbook_dir }}/ansible/jobs/llamacpp-rpc.nomad"
    dest: "/tmp/llamacpp-rpc.nomad"
    mode: '0644'
  vars:
    llm_models_var: "{{ llm_models }}"

- name: Deploy rendered llama.cpp RPC service to Nomad
  ansible.builtin.command:
    cmd: "nomad job run -detach /tmp/llamacpp-rpc.nomad"
  register: llama_job_run
  changed_when: "'submitted successfully' in llama_job_run.stdout"
  failed_when: llama_job_run.rc != 0 and 'already running' not in llama_job_run.stderr

- name: Clean up temporary rendered job file
  ansible.builtin.file:
    path: "/tmp/llamacpp-rpc.nomad"
    state: absent

- name: Wait for llama.cpp API service to be healthy in Consul
  ansible.builtin.uri:
    url: "http://127.0.0.1:8500/v1/health/service/llama-cpp-api"
    method: GET
    return_content: yes
    status_code: 200
  register: service_health
  until: "service_health.json is defined and service_health.json | length > 0 and (service_health.json[0].Checks | selectattr('Status', 'equalto', 'passing') | list | length) > 0"
  retries: 30 # Wait up to 5 minutes
  delay: 10
  ignore_errors: yes # Fail gracefully in the next task

- name: Fail if llama.cpp service did not become healthy
  ansible.builtin.fail:
    msg: "The llama.cpp API service did not become healthy in Consul after waiting. Cannot deploy pipecat app."
  when: service_health.json is not defined or service_health.json | length == 0 or not (service_health.json[0].Checks | selectattr('Status', 'equalto', 'passing') | list | length > 0)
