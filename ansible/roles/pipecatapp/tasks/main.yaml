- name: Read and parse benchmark results
  block:
    - name: Read benchmark log file
      ansible.builtin.slurp:
        src: /var/log/llama_benchmarks.jsonl
      register: benchmark_log_raw

    - name: Parse benchmark JSONL content safely using shell and jq
      ansible.builtin.shell:
        executable: /bin/bash
        cmd: |
          set -o pipefail
          echo '{{ benchmark_log_raw.content }}' | base64 --decode | while IFS= read -r line; do
            # Skip empty or whitespace-only lines
            if [[ -n "${line// /}" ]]; then
              # Check if the line is valid JSON and print it if it is
              if echo "$line" | jq -e . >/dev/null 2>&1; then
                echo "$line"
              fi
            fi
          done | jq -s .
      register: parsed_json
      changed_when: false
      when: benchmark_log_raw.content is defined

    - name: Set benchmark_results fact from parsed content
      ansible.builtin.set_fact:
        benchmark_results: "{{ (parsed_json.stdout | from_json) if (parsed_json.stdout is defined and parsed_json.stdout) else [] }}"
      when: benchmark_log_raw.content is defined
  rescue:
    - name: Set empty benchmark results on failure
      ansible.builtin.set_fact:
        benchmark_results: []

- name: Identify successful models from benchmarks
  ansible.builtin.set_fact:
    cacheable: true
    successful_models: "{{ benchmark_results | selectattr('status', 'equalto', 'success') | map(attribute='model') | list }}"

- name: Identify verified experts by checking their models against benchmark results
  ansible.builtin.set_fact:
    verified_experts: >-
      {{
        verified_experts | default([]) + [item]
      }}
  loop: "{{ experts }}"
  vars:
    # Get the list of model filenames for the current expert
    expert_model_files: "{{ (expert_models[item] | default([])) | map(attribute='filename') | list }}"
    # Check if any of the expert's models are in the list of successful models
    is_verified: "{{ expert_model_files | intersect(successful_models) | length > 0 }}"
  when: is_verified

- name: Fallback and warn if no verified experts found
  ansible.builtin.debug:
    msg: >
      ⚠️  No verified experts found! Skipping expert job creation.
      (Check /var/log/llama_benchmarks.jsonl for failed benchmarks.)
  when: verified_experts is not defined or verified_experts | length == 0


- name: Ensure correct ownership of the application directory
  ansible.builtin.file:
    path: "{{ pipecat_app_dir }}"
    owner: "{{ target_user }}"
    group: "{{ target_user }}"
    recurse: yes
  become: yes

- name: Copy requirements.txt
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/requirements.txt"
    dest: "{{ pipecat_app_dir }}/requirements.txt"
  become: yes
  tags:
    - deploy_app

- name: Install python dependencies
  ansible.builtin.pip:
    requirements: "{{ pipecat_app_dir }}/requirements.txt"
    virtualenv: "{{ pipecat_app_dir }}/venv"
    virtualenv_command: python3 -m venv
  become: yes
  tags:
    - deploy_app

- name: Copy pipecat app
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/app.py"
    dest: "{{ pipecat_app_dir }}/app.py"
  become: yes
  tags:
    - deploy_app

- name: Copy task supervisor
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/task_supervisor.py"
    dest: "{{ pipecat_app_dir }}/task_supervisor.py"
  become: yes
  tags:
    - deploy_app

- name: Copy agent factory
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/agent_factory.py"
    dest: "{{ pipecat_app_dir }}/agent_factory.py"
  become: yes
  tags:
    - deploy_app

- name: Copy worker agent
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/worker_agent.py"
    dest: "{{ pipecat_app_dir }}/worker_agent.py"
  become: yes
  tags:
    - deploy_app

- name: Copy pmm memory client
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/pmm_memory_client.py"
    dest: "{{ pipecat_app_dir }}/pmm_memory_client.py"
  become: yes
  tags:
    - deploy_app

- name: Copy pmm memory module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/pmm_memory.py"
    dest: "{{ pipecat_app_dir }}/pmm_memory.py"
  become: yes
  tags:
    - deploy_app

- name: Copy quality_control module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/quality_control.py"
    dest: "{{ pipecat_app_dir }}/quality_control.py"
  become: yes
  tags:
    - deploy_app

- name: Copy durable_execution module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/durable_execution.py"
    dest: "{{ pipecat_app_dir }}/durable_execution.py"
  become: yes
  tags:
    - deploy_app

- name: Copy workflow directory
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/workflow"
    dest: "{{ pipecat_app_dir }}/"
  become: yes
  tags:
    - deploy_app

- name: Fetch Consul management token from controller node
  ansible.builtin.slurp:
    src: /etc/consul.d/management_token
  register: consul_token_file_remote
  delegate_to: "{{ groups['controller_nodes'][0] }}"
  become: yes
  ignore_errors: yes
  no_log: true
  when: groups['controller_nodes'] is defined and groups['controller_nodes'] | length > 0

- name: Set Consul token fact from file
  ansible.builtin.set_fact:
    consul_bootstrap_token: "{{ consul_token_file_remote['content'] | b64decode | trim }}"
  when: consul_token_file_remote is defined and consul_token_file_remote is success and consul_token_file_remote.content is defined and (consul_bootstrap_token is not defined or consul_bootstrap_token == '')
  no_log: true

- name: Set Consul token from environment (fallback)
  ansible.builtin.set_fact:
    consul_bootstrap_token: "{{ lookup('env', 'CONSUL_HTTP_TOKEN') }}"
  when: (consul_bootstrap_token is not defined or consul_bootstrap_token == '')
  no_log: true

- name: Fail if Consul token is missing
  ansible.builtin.fail:
    msg: "Consul bootstrap token is missing. Please ensure /etc/consul.d/management_token exists on the controller node or export CONSUL_HTTP_TOKEN in your environment."
  when: consul_bootstrap_token is not defined or consul_bootstrap_token == ''

- name: Create pipecat environment file from template
  ansible.builtin.template:
    src: pipecat.env.j2
    dest: "{{ pipecat_app_dir }}/pipecat.env"
    mode: '0644'
  become: yes
  vars:
    llama_api_service_name: "router-api"

- name: Copy pipecat startup script
  ansible.builtin.template:
    src: start_pipecatapp.sh.j2
    dest: "{{ pipecat_app_dir }}/start_pipecat.sh"
    mode: '0755'
  become: yes
  tags:
    - deploy_app

- name: Copy memory module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/memory.py"
    dest: "{{ pipecat_app_dir }}/memory.py"
  become: yes
  tags:
    - deploy_app

- name: Copy net_utils module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/net_utils.py"
    dest: "{{ pipecat_app_dir }}/net_utils.py"
  become: yes
  tags:
    - deploy_app

- name: Copy models module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/models.py"
    dest: "{{ pipecat_app_dir }}/models.py"
  become: yes
  tags:
    - deploy_app

- name: Copy rate_limiter module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/rate_limiter.py"
    dest: "{{ pipecat_app_dir }}/rate_limiter.py"
  become: yes
  tags:
    - deploy_app

- name: Copy security module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/security.py"
    dest: "{{ pipecat_app_dir }}/security.py"
  become: yes
  tags:
    - deploy_app

- name: Copy api_keys module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/api_keys.py"
    dest: "{{ pipecat_app_dir }}/api_keys.py"
  become: yes
  tags:
    - deploy_app

- name: Copy llm_clients module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/llm_clients.py"
    dest: "{{ pipecat_app_dir }}/llm_clients.py"
  become: yes
  tags:
    - deploy_app

- name: Copy archivist service
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/archivist_service.py"
    dest: "{{ pipecat_app_dir }}/archivist_service.py"
  become: yes
  tags:
    - deploy_app

- name: Copy archivist startup script
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/start_archivist.sh"
    dest: "{{ pipecat_app_dir }}/start_archivist.sh"
    mode: '0755'
  become: yes
  tags:
    - deploy_app

- name: Copy web_server module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/web_server.py"
    dest: "{{ pipecat_app_dir }}/web_server.py"
  become: yes
  tags:
    - deploy_app

- name: Copy static assets
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/static"
    dest: "{{ pipecat_app_dir }}/"
  become: yes
  tags:
    - deploy_app

- name: Copy tools directory
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/tools"
    dest: "{{ pipecat_app_dir }}/"
  become: yes
  tags:
    - deploy_app

- name: Copy moondream_detector module
  ansible.builtin.copy:
    src: "{{ inventory_dir }}/pipecatapp/moondream_detector.py"
    dest: "{{ pipecat_app_dir }}/moondream_detector.py"
  become: yes
  tags:
    - deploy_app

- name: Ensure prompts directory exists
  ansible.builtin.file:
    path: "{{ pipecat_app_dir }}/prompts"
    state: directory
    owner: "{{ target_user }}"
    group: "{{ target_user }}"
    mode: '0755'
  become: yes
  tags:
    - deploy_app

- name: Template prompt files
  ansible.builtin.template:
    src: "prompts/{{ item }}.j2"
    dest: "{{ pipecat_app_dir }}/prompts/{{ item }}"
    mode: '0644'
    owner: "{{ target_user }}"
    group: "{{ target_user }}"
  with_items:
    - coding_expert.txt
    - creative_expert.txt
    - cynic_expert.txt
    - router.txt
    - tron_agent.txt
  become: yes
  tags:
    - deploy_app

- name: Ensure workflows directory exists
  ansible.builtin.file:
    path: "{{ pipecat_app_dir }}/workflows"
    state: directory
    owner: "{{ target_user }}"
    group: "{{ target_user }}"
    mode: '0755'
  become: yes
  tags:
    - deploy_app

- name: Template workflow files
  ansible.builtin.template:
    src: "workflows/default_agent_loop.yaml.j2"
    dest: "{{ pipecat_app_dir }}/workflows/default_agent_loop.yaml"
    mode: '0644'
    owner: "{{ target_user }}"
    group: "{{ target_user }}"
  become: yes
  tags:
    - deploy_app

#- name: Setup STT providers
#  ansible.builtin.include_role:
#    name: whisper_cpp

- name: Create VAD model directory
  ansible.builtin.file:
    path: "/home/{{ target_user }}/.cache/torch/hub/snakers4_silero-vad_master"
    state: directory
    owner: "{{ target_user }}"
    group: "{{ target_user }}"
    mode: '0755'
  become: yes

- name: Clone silero-vad model
  ansible.builtin.git:
    repo: 'https://github.com/snakers4/silero-vad.git'
    dest: "/home/{{ target_user }}/.cache/torch/hub/snakers4_silero-vad_master"
    version: master
  become: yes
  become_user: "{{ target_user }}"


- name: Build pipecatapp Docker image
  ansible.builtin.include_tasks:
    file: "{{ inventory_dir }}/ansible/tasks/build_pipecatapp_image.yaml"

- name: Inspect pipecatapp:local image
  ansible.builtin.command: docker image inspect pipecatapp:local
  register: pipecatapp_local_image_info
  changed_when: false
  become: yes

- name: Check if local registry is reachable (Pipecat App)
  ansible.builtin.wait_for:
    host: "{{ docker_registry_host }}"
    port: "{{ docker_registry_port }}"
    state: started
    timeout: 1
  register: pipecatapp_registry_check
  ignore_errors: true

- name: Set pipecatapp_image_ref (Registry)
  ansible.builtin.set_fact:
    pipecatapp_image_ref: "{{ docker_registry_host }}:{{ docker_registry_port }}/pipecatapp:local"
  when: pipecatapp_registry_check is success

- name: Set pipecatapp_image_ref (Local Fallback)
  ansible.builtin.set_fact:
    pipecatapp_image_ref: "pipecatapp:local"
  when: pipecatapp_registry_check is failed

- name: Ensure Nomad jobs directory exists
  ansible.builtin.file:
    path: "{{ nomad_jobs_dir }}"
    state: directory
    mode: '0755'
  become: yes

- name: Copy pipecatapp Nomad job file
  ansible.builtin.template:
    src: pipecatapp.nomad.j2
    dest: "{{ nomad_jobs_dir }}/pipecatapp.nomad"
  become: yes

- name: Template prompt evolution Nomad job file
  ansible.builtin.template:
    src: ../../jobs/evolve-prompt.nomad.j2
    dest: "{{ nomad_jobs_dir }}/evolve-prompt.nomad"
  become: yes

- name: Template archivist Nomad job file
  ansible.builtin.template:
    src: archivist.nomad.j2
    dest: "{{ nomad_jobs_dir }}/archivist.nomad"
  become: yes

- name: Stop and purge any existing expert jobs to ensure idempotency
  ansible.builtin.command:
    cmd: /usr/local/bin/nomad job stop -purge "expert-{{ item }}"
  loop: "{{ experts }}"
  register: expert_job_stop_status
  changed_when: "'Running' in expert_job_stop_status.stdout"
  failed_when: false
  environment:
    NOMAD_ADDR: "http://{{ advertise_ip }}:{{ nomad_http_port }}"

- name: Debug Nomad namespace
  debug:
    var: nomad_namespace

- name: Create router job file from template
  ansible.builtin.template:
    src: ../../jobs/router.nomad.j2
    dest: "{{ nomad_jobs_dir }}/router.nomad"
  become: yes

- name: Create expert job files from template for verified models
  ansible.builtin.template:
    src: ../../jobs/expert.nomad.j2
    dest: "{{ nomad_jobs_dir }}/expert-{{ current_expert }}.nomad"
  loop: "{{ verified_experts | default([]) }}"
  loop_control:
    loop_var: current_expert # Using the previously updated loop variable name
  when: verified_experts is defined and verified_experts | length > 0
  vars:
    # Define ONLY loop-specific variables here
    nomad_namespace: "{{ nomad_namespace }}"
    job_name: "expert-{{ current_expert }}"
    service_name: "expert-api-{{ current_expert }}"
    model_list: "{{ expert_models[current_expert] | default([]) }}" # Corrected default
    worker_count: 1
    rpc_pool_job_name: "llamacpp-rpc-pool"
    # DO NOT redefine ansible_memtotal_mb here
    # The benchmark data calculation can stay if needed, adjust variable names if necessary
    benchmark_data_for_expert: >-
      {{
        (benchmark_results |
        selectattr('model', 'in', expert_models[current_expert] | map(attribute='filename') | list) |
        list) | default([])
      }}
    avg_tokens: >-
      {{
        (benchmark_data_for_expert | map(attribute='tokens_per_second') | list | sum / (benchmark_data_for_expert | length))
        if (benchmark_data_for_expert | length > 0) else 0
      }}
    # expert_tags definition can stay, but ensure it doesn't redefine ansible_memtotal_mb
    current_expert_tags: >-
      [
        "expert={{ current_expert }}",
        "avg_tps={{ avg_tokens | float | round(2) }}",
        "memory_mb={{ ansible_memtotal_mb }}", # Access global fact directly
        "models={{ model_list | map(attribute='filename') | join(',') }}"
      ]

- name: Fallback and warn if no verified experts found
  ansible.builtin.debug:
    msg: >
      ⚠️  No verified experts found! Skipping expert job creation.
      (Check /var/log/llama_benchmarks.jsonl for failed benchmarks.)
  when: verified_experts is not defined or verified_experts | length == 0

- name: Copy test runner Nomad job template
  ansible.builtin.copy:
    src: ../../jobs/test-runner.nomad.j2
    dest: "{{ nomad_jobs_dir }}/test-runner.nomad.j2"
  become: yes

- name: Install Playwright browsers using the virtual environment's python
  ansible.builtin.command:
    cmd: "{{ pipecat_app_dir }}/venv/bin/python -m playwright install"
  become: yes
  become_user: "{{ target_user }}"
  async: 900 # Set timeout to 15 minutes
  poll: 10   # Check status every 10 seconds

- name: Flush handlers to apply systemd changes
  meta: flush_handlers

- name: Wait for Nomad API to be ready
  ansible.builtin.uri:
    url: "http://{{ cluster_ip }}:{{ nomad_http_port }}/v1/status/leader"
    method: GET
    status_code: 200
  register: nomad_api_status
  until: nomad_api_status.status == 200
  retries: 12 # Total wait time = retries * delay = 60 seconds
  delay: 5   # Wait 5 seconds between retries

- name: Stop and purge any existing pipecat-app job to ensure idempotency
  ansible.builtin.command:
    cmd: /usr/local/bin/nomad job stop -purge pipecat-app
  register: pipecat_job_stop_status
  changed_when: "pipecat_job_stop_status.rc == 0"
  failed_when: false
  environment:
    NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"

- name: Flush handlers to ensure service is enabled and started
  ansible.builtin.meta: flush_handlers

- name: "Archivist : Deploy Archivist Job"
  block:
    - name: "Archivist : Purge existing nomad job (force image update)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad job stop -purge archivist"
      ignore_errors: yes
      changed_when: true
      become: no

    - name: "Archivist : Run nomad job"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad job run {{ nomad_jobs_dir }}/archivist.nomad"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      changed_when: true
      become: no
      register: archivist_job_run
  rescue:
    - name: "Archivist : Get Archivist job status"
      ansible.builtin.command:
        cmd: /usr/local/bin/nomad job status -verbose archivist
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: arch_job_status
      failed_when: false
      become: no

    - name: "Archivist : Display Archivist job status"
      ansible.builtin.debug:
        var: arch_job_status.stdout

    - name: "Archivist : Get Archivist allocations"
      ansible.builtin.command:
        cmd: /usr/local/bin/nomad job allocs -json archivist
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: arch_allocs
      failed_when: false
      become: no

    - name: "Archivist : Save Archivist allocations to temp file"
      ansible.builtin.copy:
        content: "{{ arch_allocs.stdout }}"
        dest: "/tmp/arch_allocs.json"

    - name: "Archivist : Extract Allocation ID"
      ansible.builtin.command: jq -r 'sort_by(.CreateTime) | reverse | .[0].ID' /tmp/arch_allocs.json
      register: arch_alloc_id
      failed_when: false

    - name: "Archivist : Fetch Archivist logs (stdout)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs {{ arch_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: arch_logs_stdout
      failed_when: false
      become: no
      when: arch_alloc_id.stdout | length > 0

    - name: "Archivist : Display Archivist logs (stdout)"
      ansible.builtin.debug:
        var: arch_logs_stdout.stdout
      when: arch_logs_stdout.stdout is defined

    - name: "Archivist : Fetch Archivist logs (stderr)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs -stderr {{ arch_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: arch_logs_stderr
      failed_when: false
      become: no
      when: arch_alloc_id.stdout | length > 0

    - name: "Archivist : Display Archivist logs (stderr)"
      ansible.builtin.debug:
        var: arch_logs_stderr.stdout
      when: arch_logs_stderr.stdout is defined

    - name: "Archivist : Clean up temp file"
      ansible.builtin.file:
        path: "/tmp/arch_allocs.json"
        state: absent

    - name: "Archivist : Fail after debugging"
      ansible.builtin.fail:
        msg: "Archivist job failed to start. See logs above."

- name: "Router : Deploy Router Job"
  block:
    - name: "Router : Purge existing nomad job (force image update)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad job stop -purge router"
      ignore_errors: yes
      changed_when: true
      become: no

    - name: "Router : Run nomad job"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad job run {{ nomad_jobs_dir }}/router.nomad"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      changed_when: true
      become: no
      register: router_job_run
  rescue:
    - name: "Router : Display job run error"
      ansible.builtin.debug:
        var: router_job_run

    - name: "Router : Get Router job status"
      ansible.builtin.command:
        cmd: /usr/local/bin/nomad job status -verbose router
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: router_job_status
      failed_when: false
      become: no

    - name: "Router : Display Router job status"
      ansible.builtin.debug:
        var: router_job_status.stdout

    - name: "Router : Get Router allocations"
      ansible.builtin.command:
        cmd: /usr/local/bin/nomad job allocs -json router
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: router_allocs
      failed_when: false
      become: no

    - name: "Router : Save Router allocations to temp file"
      ansible.builtin.copy:
        content: "{{ router_allocs.stdout }}"
        dest: "/tmp/router_allocs.json"

    - name: "Router : Extract Allocation ID"
      ansible.builtin.command: jq -r 'sort_by(.CreateTime) | reverse | .[0].ID' /tmp/router_allocs.json
      register: router_alloc_id
      failed_when: false

    - name: "Router : Fetch litellm-proxy logs (stdout)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs -task litellm-proxy {{ router_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: litellm_logs_stdout
      failed_when: false
      become: no
      when: router_alloc_id.stdout | length > 0

    - name: "Router : Display litellm-proxy logs (stdout)"
      ansible.builtin.debug:
        var: litellm_logs_stdout.stdout
      when: litellm_logs_stdout.stdout is defined

    - name: "Router : Fetch litellm-proxy logs (stderr)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs -stderr -task litellm-proxy {{ router_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: litellm_logs_stderr
      failed_when: false
      become: no
      when: router_alloc_id.stdout | length > 0

    - name: "Router : Display litellm-proxy logs (stderr)"
      ansible.builtin.debug:
        var: litellm_logs_stderr.stdout
      when: litellm_logs_stderr.stdout is defined

    - name: "Router : Fetch llama-server-router logs (stdout)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs -task llama-server-router {{ router_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: llama_logs_stdout
      failed_when: false
      become: no
      when: router_alloc_id.stdout | length > 0

    - name: "Router : Display llama-server-router logs (stdout)"
      ansible.builtin.debug:
        var: llama_logs_stdout.stdout
      when: llama_logs_stdout.stdout is defined

    - name: "Router : Fetch llama-server-router logs (stderr)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs -stderr -task llama-server-router {{ router_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: llama_logs_stderr
      failed_when: false
      become: no
      when: router_alloc_id.stdout | length > 0

    - name: "Router : Display llama-server-router logs (stderr)"
      ansible.builtin.debug:
        var: llama_logs_stderr.stdout
      when: llama_logs_stderr.stdout is defined

    - name: "Router : Clean up temp file"
      ansible.builtin.file:
        path: "/tmp/router_allocs.json"
        state: absent

    - name: "Router : Fail after debugging"
      ansible.builtin.fail:
        msg: "Router job failed to start. See logs above."

- name: Ensure Nomad service registration (Fallback)
  block:
    - name: Check if router service is registered in Consul
      ansible.builtin.uri:
        url: "http://{{ cluster_ip }}:{{ consul_http_port }}/v1/catalog/service/router-api"
        headers:
          X-Consul-Token: "{{ consul_bootstrap_token | default(omit) }}"
        return_content: yes
      register: router_catalog_check
      changed_when: false

    - name: Manually register router service in Consul if missing
      ansible.builtin.uri:
        url: "http://{{ cluster_ip }}:{{ consul_http_port }}/v1/agent/service/register"
        method: PUT
        headers:
          X-Consul-Token: "{{ consul_bootstrap_token | default(omit) }}"
        body_format: json
        body:
          Name: "router-api"
          Address: "{{ cluster_ip }}"
          Port: "{{ router_port }}"
          Tags: ["router", "manual-registration"]
          Check:
            Name: "Router API Health"
            HTTP: "http://{{ cluster_ip }}:{{ router_port }}/health"
            Interval: "15s"
            Timeout: "5s"
        status_code: [200, 204]
      when: >
        (router_catalog_check.json is defined and router_catalog_check.json | length == 0)

- name: Wait for the router service to be healthy in Consul
  ansible.builtin.uri:
    url: "http://{{ cluster_ip }}:{{ consul_http_port }}/v1/health/service/router-api"
    headers:
      X-Consul-Token: "{{ consul_bootstrap_token | default(omit) }}"
    return_content: yes
  register: router_health
  until: >
    router_health.json is defined and
    router_health.json | map(attribute='Checks') | flatten | selectattr('Status', 'equalto', 'passing') | list | length > 0
  retries: 60
  delay: 10
  changed_when: false

- name: "Pipecat App : Deploy Pipecat Job"
  block:
    - name: "Pipecat App : Run nomad job"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad job run -detach {{ nomad_jobs_dir }}/pipecatapp.nomad"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      changed_when: true
      become: no
      register: pipecat_job_run

    - name: Wait for the pipecat-app service to be healthy in Consul
      ansible.builtin.uri:
        url: "http://{{ cluster_ip }}:{{ consul_http_port }}/v1/health/service/pipecat-app"
        headers:
          X-Consul-Token: "{{ consul_bootstrap_token | default(omit) }}"
        return_content: yes
      register: pipecat_health
      until: >
        pipecat_health.json is defined and
        pipecat_health.json | map(attribute='Checks') | flatten | selectattr('Status', 'equalto', 'passing') | list | length > 0
      retries: 60
      delay: 10
      changed_when: false
  rescue:
    - name: "Pipecat App : Get Pipecat App job status"
      ansible.builtin.command:
        cmd: /usr/local/bin/nomad job status -verbose pipecat-app
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: pipecat_job_status
      failed_when: false
      become: no

    - name: "Pipecat App : Display Pipecat App job status"
      ansible.builtin.debug:
        var: pipecat_job_status.stdout

    - name: "Pipecat App : Get Pipecat App allocations"
      ansible.builtin.command:
        cmd: /usr/local/bin/nomad job allocs -json pipecat-app
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: pipecat_allocs
      failed_when: false
      become: no

    - name: "Pipecat App : Save Pipecat App allocations to temp file"
      ansible.builtin.copy:
        content: "{{ pipecat_allocs.stdout }}"
        dest: "/tmp/pipecat_allocs.json"

    - name: "Pipecat App : Extract Allocation ID"
      ansible.builtin.command: jq -r 'sort_by(.CreateTime) | reverse | .[0].ID' /tmp/pipecat_allocs.json
      register: pipecat_alloc_id
      failed_when: false

    - name: "Pipecat App : Fetch Pipecat App logs (stdout)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs {{ pipecat_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: pipecat_logs_stdout
      failed_when: false
      become: no
      when: pipecat_alloc_id.stdout | length > 0

    - name: "Pipecat App : Display Pipecat App logs (stdout)"
      ansible.builtin.debug:
        var: pipecat_logs_stdout.stdout
      when: pipecat_logs_stdout.stdout is defined

    - name: "Pipecat App : Fetch Pipecat App logs (stderr)"
      ansible.builtin.command:
        cmd: "/usr/local/bin/nomad alloc logs -stderr {{ pipecat_alloc_id.stdout }}"
      environment:
        NOMAD_ADDR: "http://{{ cluster_ip }}:{{ nomad_http_port }}"
      register: pipecat_logs_stderr
      failed_when: false
      become: no
      when: pipecat_alloc_id.stdout | length > 0

    - name: "Pipecat App : Display Pipecat App logs (stderr)"
      ansible.builtin.debug:
        var: pipecat_logs_stderr.stdout
      when: pipecat_logs_stderr.stdout is defined

    - name: "Pipecat App : Clean up temp file"
      ansible.builtin.file:
        path: "/tmp/pipecat_allocs.json"
        state: absent

    - name: "Pipecat App : Fail after debugging"
      ansible.builtin.fail:
        msg: "Pipecat App job failed to start. See logs above."
