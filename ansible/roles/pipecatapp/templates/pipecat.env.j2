#!/bin/sh
export LLAMA_API_SERVICE_NAME="{{ llama_api_service_name | default('llamacpp-rpc-api') }}"
export USE_SUMMARIZER="{{ use_summarizer | default('false') }}"
export STT_SERVICE="{{ stt_service | default('faster-whisper') }}"
export LLM_PROVIDER="{{ llm_provider | default('local') }}"
export LLM_MODEL="{{ llm_model | default('') }}"
export PIECAT_API_KEYS="{{ pipecat_api_keys }}"
export EXTERNAL_EXPERTS_CONFIG='{{ external_experts_config | to_json }}'
export OPENAI_API_KEY="{{ openai_api_key | default('') }}"
export GROQ_API_KEY="{{ groq_api_key | default('') }}"
export DEEPSEEK_API_KEY="{{ deepseek_api_key | default('') }}"
export OPENROUTER_API_KEY="{{ openrouter_api_key | default('') }}"
# Use the memory service exposed on the cluster IP (port 8000 mapped in memory_service.nomad.j2)
export MEMORY_SERVICE_URL="http://{{ cluster_ip }}:8000"

export CONSUL_HOST="{{ cluster_ip }}"
export CONSUL_PORT="{{ consul_http_port }}"
export CONSUL_HTTP_TOKEN="{{ consul_bootstrap_token | default('') }}"
export ROUTER_PORT="{{ router_port }}"
export LLAMA_API_URL="http://{{ cluster_ip }}:{{ router_port }}/v1"
export WEB_PORT="${WEB_PORT:-{{ nanochat_port | default(8000) }}}"
export YOLO_MODEL_PATH="{{ nomad_models_dir }}/vision/yolov8n.pt"
export PRIMA_API_SERVICE_NAME="llama-api-main"
