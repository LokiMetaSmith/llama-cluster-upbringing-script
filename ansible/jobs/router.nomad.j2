# This Nomad job file runs the "router" service.
job "router" {
  datacenters = ["dc1"]
  type        = "service"
  namespace   = "{{ nomad_namespace | default('default') }}"
  priority    = 50

  group "router" {
    count = {{ router_count }}

    update {
      max_parallel      = 1
      progress_deadline = "15m"
      healthy_deadline  = "10m" # Give it 10 mins to become healthy
      min_healthy_time  = "30s"
      auto_revert       = true
    }

    migrate {
      max_parallel = 1
      health_check = "checks"
      healthy_deadline = "5m"
    }

    reschedule {
      attempts  = 3
      interval  = "5m"
      delay     = "30s"
      unlimited = false
    }

    network {
      mode = "host"
      port "http" {
        static = {{ router_port }}
      }
      port "ollama" {
        static = 11434
      }
    }

    service {
      name     = "router-api"
      provider = "consul"
      port     = "http"

      check {
        type         = "http"
        port         = "http"
        path         = "/health"
        interval     = "15s"
        timeout      = "5s"
        address_mode = "host"
      }
    }

    service {
      name     = "ollama-api"
      provider = "consul"
      port     = "ollama"

      check {
        type         = "http"
        port         = "ollama"
        path         = "/"
        interval     = "30s"
        timeout      = "5s"
        address_mode = "host"
      }
    }

    task "litellm-proxy" {
      driver = "docker"

      config {
        image = "ghcr.io/berriai/litellm:main-latest"
        ports = ["ollama"]
        args = [
          "--port", "${NOMAD_PORT_ollama}",
          "--model", "openai/{{ expert_models.router[0].filename }}",
          "--api_base", "http://localhost:{{ router_port }}/v1",
          "--drop_params"
        ]
      }

      env {
        LITELLM_MASTER_KEY = "sk-1234"
      }

      resources {
        cpu    = 500
        memory = 512
      }
    }

    task "llama-server-router" {
      driver = "raw_exec"

      template {
        data = <<EOH
#!/bin/bash
set -e
echo "--- Starting Router ---"

exec /usr/local/bin/llama-server \
  --model /opt/nomad/models/llm/{{ expert_models.router[0].filename }} \
  --host 0.0.0.0 \
  --port ${NOMAD_PORT_http} \
  --n-gpu-layers 99 \
  --mlock \
  --verbose
EOH
        destination = "local/run_router.sh"
        perms       = "0755"
      }

      config {
        command = "local/run_router.sh"
      }

      resources {
        cpu    = 2000
        memory = 4096 # Allocate a fixed amount of memory for the router
      }
    }
  }
}
