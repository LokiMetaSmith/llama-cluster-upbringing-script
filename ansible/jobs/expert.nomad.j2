# This Nomad job file runs the main "expert" orchestrator service.
# It loads a single, suitable model and distributes inference across a
# pool of rpc-server providers discovered via Consul.

variable "job_name" {
  type = string
}

variable "service_name" {
  type = string
}

variable "rpc_pool_job_name" {
  type = string
}

# --- DYNAMIC MODEL SELECTION & TPS CALCULATION ---
{% set memory_buffer_mb = 2048 %}
{% set available_memory_mb = (ansible_memtotal_mb | int) - memory_buffer_mb %}
{% set model_list = expert_models[current_expert] %}
{% set models_with_memory = model_list | selectattr('memory_mb', 'defined') %}
{% set suitable_models_unsorted = models_with_memory | selectattr('memory_mb', 'le', available_memory_mb) %}
{% set suitable_models = suitable_models_unsorted | sort(attribute='memory_mb', reverse=true) | list %}
{% set best_model = suitable_models[0] %}
{% set best_model_sanitized = best_model.filename | regex_replace('[^a-zA-Z0-9-]', '-') %}

job "{{ job_name }}" {
  datacenters = ["dc1"]

  group "{{ job_name }}-group" {
    count = 1

    scaling {
      enabled = true
      min     = 1
      max     = 10

      policy {
        cooldown = "1m"
        evaluation_interval = "30s"

        check "cpu_usage" {
          source = "nomad"
          query  = "avg(nomad.client.allocs.cpu.total_percent)"

          strategy "target-value" {
            target = 70
          }
        }
      }
    }

    update {
      max_parallel = 1
      progress_deadline = "60m"
      min_healthy_time = "10s"
      healthy_deadline = "45m"
      auto_revert = true
    }

    network {
      mode = "host"
      port "http" {}
      port "api" {}
    }

    # The LLM API Service
    service {
      name = "{{ service_name }}"
      tags = [{{ current_expert_tags | map('to_json') | join(', ') }}]
      port = "api"

      check {
        type     = "http"
        path     = "/health"
        interval = "15s"
        timeout  = "10s"
        
        # Give the app 90 seconds to load heavy models before restarting
        check_restart {
          limit = 3
          grace = "90s"
        }
      }
    }

    # The Pipecat Agent Service
    service {
      name = "{{ service_name }}-agent"
      port = "http"
      tags = ["agent", "expert={{ current_expert }}"]

      check {
        type     = "http"
        path     = "/health"
        interval = "15s"
        timeout  = "10s"
      }
    }

    task "llama-server" {
      driver = "raw_exec"

      constraint {
        attribute = "${meta.node_type}"
        value     = "controller"
      }

      template {
        data = <<EOH
#!/bin/bash
set -e
echo "Starting llama-server for {{ best_model.filename }}..."

# Discover RPC servers
RPC_SERVERS=""
{% raw %}
{{ range service "rpc-{% endraw %}{{ best_model_sanitized }}{% raw %}-provider" }}
RPC_SERVERS="$RPC_SERVERS,{{ .Address }}:{{ .Port }}"
{{ end }}
{% endraw %}
# Remove leading comma
RPC_SERVERS="$${RPC_SERVERS#,}"

if [ -z "$RPC_SERVERS" ]; then
  echo "Warning: No RPC servers found for model {{ best_model.filename }}. Waiting..."
  # Just sleep to avoid crash loops if RPC servers are starting up
  sleep 10
  exit 1
fi

echo "Found RPC servers: $RPC_SERVERS"

exec /usr/local/bin/llama-server \
    --model /opt/nomad/models/llm/{{ best_model.filename }} \
    --host 0.0.0.0 \
    --port ${NOMAD_PORT_api} \
    --rpc "$RPC_SERVERS" \
    --n-gpu-layers 99 \
    --mlock \
    --verbose
EOH
        destination = "local/run_server.sh"
        perms = "0755"
      }

      config {
        command = "local/run_server.sh"
      }

      resources {
        cpu    = 1000
        memory = 1024
      }
    }

    task "{{ job_name }}" {
      driver = "docker"

      constraint {
        attribute = "${meta.node_type}"
        value     = "controller"
      }

      config {
        image = "{{ pipecat_image_id }}"
        force_pull = false
        ports = ["http"]
        command = "/bin/bash"
        args = ["-c", "if [ -f /opt/pipecatapp/start_pipecat.sh ]; then exec /opt/pipecatapp/start_pipecat.sh; else exec /opt/start_pipecat.sh; fi"]
        
        mounts = [
          {
            type = "bind"
            target = "/opt/pipecatapp"
            source = "{{ pipecat_app_mount_source | default(pipecat_app_dir) }}"
            readonly = false
          },
          # FIX: Mount the models directory so the app can find Whisper and Embeddings
          {
            type = "bind"
            target = "/opt/nomad/models"
            source = "{{ nomad_models_dir | default('/home/loki/models') }}"
            readonly = true
          }
        ]
      }

      env {
        STT_SERVICE = "faster-whisper"
        WEB_PORT = "${NOMAD_PORT_http}"

        # Point to the local llama-server we just started
        LLAMA_API_BASE_URL = "http://localhost:${NOMAD_PORT_api}/v1"
        LLAMA_API_SERVICE_NAME = "{{ service_name }}" # For discovery fallback

        PIPECAT_API_KEYS = "{{ pipecat_api_keys | default('') }}"

        # Necessary for service discovery
        CONSUL_HOST = "${attr.unique.network.ip-address}"
        CONSUL_PORT = "8500"
        CONSUL_HTTP_ADDR = "http://${attr.unique.network.ip-address}:8500"
        CONSUL_HTTP_TOKEN = "{{ consul_bootstrap_token | default('') }}"
      }

      resources {
        cpu    = 500 # 500 MHz
        memory = {{ pipecat_memory_mb | default(4096) }} # 4096MB
      }
    }
  }
}
