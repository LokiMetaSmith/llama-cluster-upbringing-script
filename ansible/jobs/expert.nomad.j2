# This Nomad job file runs the main "expert" orchestrator service.
# It loads a single, suitable model and distributes inference across a
# pool of rpc-server providers discovered via Consul.

variable "job_name" {
  type = string
}

variable "service_name" {
  type = string
}

variable "rpc_pool_job_name" {
  type = string
}

# --- DYNAMIC MODEL SELECTION & TPS CALCULATION ---
{% set memory_buffer_mb = 2048 %}
{% set available_memory_mb = (ansible_memtotal_mb | int) - memory_buffer_mb %}
{% set model_list = expert_models[current_expert] %}
{% set models_with_memory = model_list | selectattr('memory_mb', 'defined') %}
{% set suitable_models_unsorted = models_with_memory | selectattr('memory_mb', 'le', available_memory_mb) %}
{% set suitable_models = suitable_models_unsorted | sort(attribute='memory_mb', reverse=true) | list %}
{% set rpc_service_name = rpc_pool_job_name | default('llamacpp-rpc-pool') ~ '-' ~ current_expert ~ '-provider' %}
{% set args_list = ["/opt/pipecatapp/supervisor.py", "--expert-id=" ~ job_name] %}


job "{{ job_name }}" {
  datacenters = ["dc1"]

  group "{{ job_name }}-group" {
    count = 1

    update {
      max_parallel = 1
      progress_deadline = "15m"
      min_healthy_time = "10s"
      healthy_deadline = "10m"
      auto_revert = true
    }

    migrate {
      max_parallel = 1
      health_check = "checks"
      healthy_deadline = "5m"
    }

    reschedule {
      attempts  = 3
      interval  = "1m"
      delay     = "30s"
      unlimited = false
    }

    network {
      port "http" {
        to = 8000
      }
    }

    service {
      name = "{{ service_name }}"
      tags = [{{ current_expert_tags | map('to_json') | join(', ') }}]
      port = "http"

      check {
        type     = "http"
        path     = "/health"
        interval = "10s"
        timeout  = "10s"
      }
    }

    task "{{ job_name }}" {
      driver = "docker"

      config {
        image = "pipecatapp:local"
        force_pull = false
        ports = ["http"]
      }

      env {
        STT_SERVICE = "faster-whisper"
        LLAMA_API_SERVICE_NAME = "{{ rpc_pool_job_name }}-provider"
      }

      resources {
        cpu    = 500 # 500 MHz
        memory = 1024 # 1024MB
      }
    }
  }
}
