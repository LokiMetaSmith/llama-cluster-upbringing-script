# This Nomad job file runs the main "expert" orchestrator service.
# It loads a single, suitable model and distributes inference across a
# pool of rpc-server providers discovered via Consul.

# --- DYNAMIC MODEL SELECTION & TPS CALCULATION ---
{% set memory_buffer_mb = 2048 %}
{% set available_memory_mb = (ansible_memtotal_mb | int) - memory_buffer_mb %}
{% set model_list = expert_models[item] %}
{% set models_with_memory = model_list | selectattr('memory_mb', 'defined') %}
{% set suitable_models_unsorted = models_with_memory | selectattr('memory_mb', 'le', available_memory_mb) %}
{% set suitable_models = suitable_models_unsorted | sort(attribute='memory_mb', reverse=true) | list %}
{% set rpc_service_name = rpc_pool_job_name | default('llamacpp-rpc-pool') ~ '-' ~ item ~ '-provider' %}
{# Calculate avg TPS from benchmark data passed from Ansible #}
{% set tps_list = benchmark_data_for_expert | map(attribute='tokens_per_second') | list %}
{% set avg_tps = (tps_list | sum) / (tps_list | length) if (tps_list | length > 0) else 0 %}

job "{{ job_name | default('expert-service') }}" {
  datacenters = ["dc1"]
  namespace   = "{{ (nomad_namespace | default('default')) | string }}" # Force string just in case
  priority    = 50

  # --- ORCHESTRATOR GROUP ---
  group "orchestrator" {
    count = 1

    network {
      mode = "host"
      port "http" { to = 8080 }
    }

    # Register the main API endpoint in Consul
    service {
      name     = "{{ service_name | default('expert-api-main') }}"
      provider = "consul"
      port     = "http"

      # --- ADD TAGS HERE ---
      tags = [
        "expert={{ job_name | replace('expert-', '') }}", # Extract expert name
        "nomad_namespace={{ nomad_namespace | default('default') }}",
        "avg_tps={{ avg_tps | round(2) }}",
        "memory_mb={{ ansible_memtotal_mb }}",
        # Use suitable_models[0] if it exists, otherwise indicate no model loaded
        "model={{ suitable_models[0].filename if suitable_models else 'none' }}"
      ]
      # --- END ADD TAGS ---

      check {
        type     = "http"
        path     = "/health"
        interval = "15s"
        timeout  = "5s"
      }
    }

    task "llama-server-orchestrator" {
      driver = "raw_exec"

      template {
        data = <<EOH
#!/bin/bash
set -e
echo "--- Starting Orchestrator for {{ job_name }} ---"

# 1. Discover RPC worker services via Consul.
worker_ips=""
for i in {1..30}; do
  worker_ips=$(curl -s "http://127.0.0.1:8500/v1/health/service/{{ rpc_service_name }}?passing" | jq -r '[.[] | .Service | "\(.Address):\(.Port)"] | join(",")')
  if [ -n "$worker_ips" ]; then
    echo "Discovered GPU Provider IPs: $worker_ips"
    break
  fi
  echo "No GPU providers found yet, retrying in 10s... (attempt $i/30)"
  sleep 10
done

if [ -z "$worker_ips" ]; then
  echo "FATAL: No RPC GPU providers became available after 5 minutes. Exiting."
  exit 1
fi

# 2. Select the best model and launch the server.
{% if suitable_models %}
  selected_model_filename="{{ suitable_models[0].filename }}"
  selected_model_name="{{ suitable_models[0].name }}"

  echo "âœ… Found suitable model. Preparing to launch server with: $selected_model_name"

  exec /usr/local/bin/llama-server \
    --model /opt/nomad/models/llm/$selected_model_filename \
    --host 0.0.0.0 \
    --port {{ '{{' }} env "NOMAD_PORT_http" {{ '}}' }} \
    --n-gpu-layers 99 \
    --mlock \
    --rpc "$worker_ips"

{% else %}
  echo "FATAL: No suitable models found for the available system memory of {{ ansible_memtotal_mb }} MB."
  exit 1
{% endif %}
EOH
        destination = "local/run_orchestrator.sh"
        perms       = "0755"
      }

      config {
        command = "local/run_orchestrator.sh"
      }

      resources {
        cpu    = 2000 # Higher CPU for the main process
        memory = {{ (suitable_models[0].memory_mb | default(4096)) + 1024 if suitable_models else 512 }} # Memory for model + buffer, or minimal if no model
      }
    }
  }
}
