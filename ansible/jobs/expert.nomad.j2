# This Nomad job starts a distributed llama.cpp RPC cluster.
# It consists of one "master" node that runs the main llama-server,
# and one or more "worker" nodes that run the rpc-server for offloading.
 
# --- DYNAMIC MODEL SELECTION ---
# This Jinja2 block filters the full list of models to only include
# those that can fit within the host system's memory, adding a 2GB buffer.
{% set memory_buffer_mb = 2048 %}
{% set available_memory_mb = ansible_memtotal_mb - memory_buffer_mb %}

{% set suitable_models = model_list | selectattr('memory_mb', 'defined') | selectattr('memory_mb', '<=', available_memory_mb) | list %}

job "expert-{{ expert_name }}" {
  datacenters = ["dc1"]
  namespace   = "default"

  meta {
    expert_name = "{{ expert_name }}"
  }

  # --- MASTER GROUP ---
  # This group runs the main llama-server which acts as the entry point.
  group "master" {
    count = 1

    volume "models" {
      type      = "host"
      source    = "models"
      read_only = true
    }
    network {
      mode = "host"
      port "http" { to = 8080 }
    }

    service {
      name     = "{{ service_name | default('prima-api-main') }}"
      provider = "consul"
      port     = "http"
      check {
        type     = "http"
        path     = "/health"
        interval = "15s"
        timeout  = "5s"
      }
    }

    task "llama-server-master" {
      driver = "raw_exec"

      template {
        data = <<EOH
#!/bin/bash
set -e
echo "--- Starting RPC Master for {{ job_name }} ---"

# 1. Discover RPC worker services via Consul
worker_ips=""
for i in {1..30}; 
do
  worker_ips=$(curl -s "http://127.0.0.1:8500/v1/health/service/{{ job_name }}-worker?passing" | jq -r '[.[] | .Service | "\\(.Address):\\(.Port)"] | join(",")')
  if [ -n "$worker_ips" ];
then
    echo "Discovered Worker IPs: $worker_ips"
    break
  fi
  echo "No workers found yet, retrying in 10s... (attempt $i/30)"
  sleep 10
done


rpc_args=""
if [ -n "$worker_ips" ]; then
  echo "Workers found. Configuring RPC."
  rpc_args="--rpc-server $worker_ips"
else
  echo "No workers found after waiting. Starting in standalone mode."
fi

health_check_url="http://127.0.0.1:{{ '{{' }} env \"NOMAD_PORT_http\" {{ '}}' }}/health"

# Loop through suitable models for failover
# This list has been pre-filtered by Jinja2 to match system memory.

{% if suitable_models %}
{% for model in suitable_models %}
  echo "Attempting to start llama-server with suitable model: {{ model.name }}"

  /usr/local/bin/llama-server \
    --model "/opt/nomad/models/llm/{{ model.filename }}" \
    --host 0.0.0.0 \
    --port {{ '{{' }} env "NOMAD_PORT_http" {{ '}}' }} \
    --n-gpu-layers 99 \
    --mlock \
    $rpc_args &

  server_pid=$!
  echo "Server process started (PID $server_pid). Waiting for health check..."

  healthy=false
  for i in {1..30};
  do
    sleep 10

    # Use curl with verbose logging to diagnose health check failures.
    # We redirect stdout to /dev/null but let stderr (where -v output goes) print.
    if curl -s --fail --verbose $health_check_url > /dev/null; then
      echo "✅ Server is healthy with model {{ model.name }}!"
      healthy=true
      break
    else
      echo "Health check failed (attempt $i/30). See verbose curl output above."
    fi
  done


  if [ "$healthy" = true ]; then
    echo "Successfully started llama-server with model: {{ model.name }}"
    # Write the active model to Consul KV for other services to discover
    curl -X PUT --data "{{ model.name }}" "http://127.0.0.1:8500/v1/kv/experts/${NOMAD_META_expert_name}/active_model"

    # If we get here, the server is healthy and running. Wait for it to exit.
    if [ -n "$server_pid" ]; then
      wait $server_pid
    fi
    exit 0
  else
    echo "❌ Server failed to become healthy with {{ model.name }}. Trying next model."
    # Only try to kill the process if the PID was captured.
    if [ -n "$server_pid" ]; then
      kill $server_pid
      wait $server_pid 2>/dev/null
    fi
  fi
{% endfor %}
{% else %}
  echo "FATAL: No suitable models found for the available system memory of {{ ansible_memtotal_mb }} MB."
{% endif %}

echo "❌ All suitable models failed to start. Exiting."
exit 1
EOH
        destination = "local/run_master.sh"
        perms       = "0755"
      }

      config {
        command = "local/run_master.sh"
      }

      resources {

        cpu    = 2000
        memory = 4096 # Allocate memory for the master process itself
      }

      volume_mount {
        volume      = "models"
        destination = "/opt/nomad/models"
        read_only   = true

      }
    }
  }

  # --- WORKER GROUP ---
  # This group runs the rpc-server processes that perform the actual inference.
  group "workers" {

    count = {{ worker_count | default(1) }}


    network {
      mode = "host"
      port "rpc" {}
    }

    service {
      name     = "{{ job_name }}-worker"
      provider = "consul"
      port     = "rpc"
      check {
        type     = "tcp"
        interval = "15s"
        timeout  = "5s"
      }
    }

    task "rpc-server-worker" {
      driver = "raw_exec"

      config {
        command = "/usr/local/bin/rpc-server"
        args = [
          "--help"
        ]
      }

      resources {
        cpu    = 500
        memory = 1024 # Memory for the RPC process itself
      }
    }
  }
}
