# Prompt Engineering Workflow

Last updated: 2025-11-06

This directory contains the tools and workflows for automatically evolving and improving the agent's core prompt using the `openevolve` library.

## Overview

This project uses an evolutionary algorithm to improve the agent's core logic. The process iteratively mutates the main `app.py` file and evaluates its performance against a test suite. This allows the system to autonomously discover and implement code changes that lead to better agent behavior and higher success rates on defined tasks.

## Workflow

1. **Define Test Cases:**
    Add or modify the YAML test cases in the `evaluation_suite/` directory. Each test case defines a scenario and the expected outcome (e.g., a specific tool call or a correct response). For failures detected at runtime, a new test case can be automatically generated by the `reflection/adaptation_manager.py` script.

2. **Run the Evolution Script:**
    Execute the `evolve.py` script. This will:
    - Take the current `app.py` as the starting point.
    - Use an LLM to generate mutations of the entire Python script.
    - For each new version of the code, run the `evaluator.py` script to calculate a fitness score based on the test suite.
    - After many iterations, it will output the best-performing version of the `app.py` code that it has found.

3. **Update the Agent:**
    Review the improved code output by the evolution script. Once you have verified that the changes are correct and beneficial, manually copy the new code into the `ansible/roles/pipecatapp/files/app.py` file.

## Running the Workflow

```bash
# Set your OpenAI API key
export OPENAI_API_KEY="your-key-here"

# Run the evolution process
python prompt_engineering/evolve.py

# To run against a specific, dynamically generated test case:
python prompt_engineering/evolve.py --test-case path/to/your/test_case.yaml
```
