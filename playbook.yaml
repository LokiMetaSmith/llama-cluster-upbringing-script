- name: Play 1 - Bootstrap New Nodes as 'root'
  hosts: all
  become: yes # We are already root

  tasks:
    - name: Ensure the sudo package is installed
      ansible.builtin.apt:
        name: sudo
        state: present

    - name: Ensure the rsync package is installed
      ansible.builtin.apt:
        name: rsync
        state: present

    - name: Ensure the sudo group exists
      ansible.builtin.group:
        name: sudo
        state: present

    - name: Create the '{{ target_user }}' account with sudo access
      ansible.builtin.user:
        name: "{{ target_user }}"
        shell: /bin/bash
        groups: sudo
        append: yes

    - name: Set correct permissions for the user's home directory
      ansible.builtin.file:
        path: "/home/{{ target_user }}"
        state: directory
        mode: '0755'

    - name: Allow '{{ target_user }}' to have passwordless sudo access
      ansible.builtin.lineinfile:
        path: "/etc/sudoers.d/ansible-{{ target_user }}-nopasswd"
        line: "{{ target_user }} ALL=(ALL) NOPASSWD: ALL"
        create: yes
        validate: 'visudo -cf %s'
        mode: '0440'


- name: Play 2 - Configure Nodes as '{{ target_user }}'
  hosts: all
  remote_user: "{{ target_user }}"
  become: yes # Now we use the sudo privileges we just created

  vars_files:
    - group_vars/all.yaml
    - group_vars/models.yaml

  pre_tasks:
    - name: Check connectivity to all nodes
      ansible.builtin.ping:

    - name: Distribute control repository to all nodes
      ansible.posix.synchronize:
        src: "{{ playbook_dir }}/"
        dest: /opt/cluster-infra/
        rsync_opts:
          - "--exclude=.git"
      when: inventory_hostname != 'localhost'

  roles:
    - system_deps
    - common

  tasks:
    - name: Run service setup roles
      include_role:
        name: "{{ role_name }}"
      loop:
        - consul
        - docker
        - nomad
      loop_control:
        loop_var: role_name

    - name: Flush handlers to ensure core services are started
      meta: flush_handlers

    - name: Wait for a Consul leader to be elected
      ansible.builtin.uri:
        url: "http://127.0.0.1:8500/v1/status/leader"
        return_content: yes
      register: consul_leader_status
      until: consul_leader_status.content != '""' and consul_leader_status.status == 200
      retries: 12
      delay: 5
      ignore_errors: yes

    - name: Fail gracefully if no Consul leader is found
      ansible.builtin.fail:
        msg: |
          The Consul service is running, but it has not elected a leader after 60 seconds.
          This usually means the `bootstrap_expect` value in your Consul configuration is
          higher than the number of server nodes available.
      when: consul_leader_status.failed or consul_leader_status.content == '""'

    - name: Populate Consul KV with configuration
      include_role:
        name: config_manager
      run_once: true

    - name: Install all python dependencies
      include_role:
        name: python_deps

    - name: Run application and configuration roles
      include_role:
        name: "{{ role_name }}"
      loop:
        - provisioning_api
        - desktop_extras
        - paddler
        - vision
        - power_manager
      loop_control:
        loop_var: role_name

    - name: Run model-dependent roles
      include_role:
        name: "{{ role_name }}"
      loop:
        - download_models
        - llama_cpp
        - whisper_cpp
      loop_control:
        loop_var: role_name
      when: not external_model_server

    - name: Flush handlers to ensure all services are started
      meta: flush_handlers

  post_tasks:
    - name: Read rendered Nomad config file
      ansible.builtin.slurp:
        src: /etc/nomad.d/client.hcl
      register: nomad_config_file

    - name: Display rendered Nomad config file
      ansible.builtin.debug:
        msg: "{{ nomad_config_file['content'] | b64decode }}"

    - name: Discover and save the MAC address for future use
      ansible.builtin.blockinfile:
        path: "host_vars/{{ inventory_hostname }}.yaml"
        create: yes
        block: |
          mac_address: "{{ ansible_default_ipv4.macaddress }}"
      delegate_to: localhost

- name: Play 3 - Final Verification
  hosts: all
  become: no
  gather_facts: no
  tasks:
    - name: Check if the Consul service is running
      ansible.builtin.systemd:
        name: consul
      register: consul_service_status
      failed_when: "consul_service_status.status.ActiveState != 'active'"
      changed_when: false
      ignore_errors: yes # We'll provide a better error message in the next task

    - name: Fail gracefully if Consul service is not running
      ansible.builtin.fail:
        msg: |
          The Consul service is not running or has failed.
          Playbook cannot continue without a functioning Consul service.
          Please check the service logs by running 'journalctl -u consul.service' on the target node.
          Status details: {{ consul_service_status }}
      when: consul_service_status.failed

    - name: Wait for a Consul leader to be elected
      ansible.builtin.uri:
        url: "http://127.0.0.1:8500/v1/status/leader"
        return_content: yes
      register: consul_leader_status
      until: consul_leader_status.content != '""' and consul_leader_status.status == 200
      retries: 12
      delay: 5
      ignore_errors: yes

    - name: Fail gracefully if no Consul leader is found
      ansible.builtin.fail:
        msg: |
          The Consul service is running, but it has not elected a leader after 60 seconds.
          This usually means the `bootstrap_expect` value in your Consul configuration is
          higher than the number of server nodes available.
          - For a single-node setup, ensure `bootstrap_expect` is 1.
          - For a multi-node setup, ensure enough controller nodes are running to form a quorum.
      when: consul_leader_status.failed or consul_leader_status.content == '""'

      # In playbook.yaml

- name: Play 4 - Deploy Core AI Services and Applications
  hosts: controller_nodes[0]
  connection: local

  pre_tasks:
    - name: Load model variables
      ansible.builtin.include_vars:
        file: group_vars/models.yaml
  vars_files:
    - group_vars/all.yaml
    - group_vars/models.yaml

  roles:
    - role: bootstrap_agent
    - role: pipecatapp
# In your main playbook.yaml, after "Play 4"
#
#- name: Play 5 - Deploy Distributed AI Experts
#  hosts: controller_nodes[0] # Run these commands from a single controller node
#  connection: local
#  gather_facts: no
#  become: no
#
  # Define the list of experts to deploy
#  vars:
#    experts:
#      - name: main
#      - name: coding
#      - name: math
#      - name: extract#

  #tasks:
#    - name: Ensure the GPU Provider Pool job is running
#      ansible.builtin.command: >
#        nomad job run -var="job_name=llamacpp-rpc-pool" -var="worker_count={{ groups['workers'] | length }}" /opt/cluster-infra/nomad/jobs/llamacpp-rpc.nomad.j2
#      changed_when: true # This command always triggers a deployment evaluation#

    #- name: Wait for GPU Providers to become healthy in Consul
    #  ansible.builtin.uri:
    #    url: "http://127.0.0.1:8500/v1/health/service/llamacpp-rpc-pool-provider?passing"
    #    return_content: yes
    #  register: consul_health
    #  until: "consul_health.json | length >= (groups['workers'] | length)"
    #  retries: 30 # Wait up to 5 minutes (30 * 10s)
    #  delay: 10
    #  vars:
    #    ansible_connection: local # Ensure this task runs on the controller#

    #- name: Deploy the Expert Orchestrator services
    #  ansible.builtin.command: >
    #    nomad job run
    #    -var="job_name=expert-{{ item.name }}"
    #    -var="service_name=expert-api-{{ item.name }}"
    #    -var="rpc_pool_job_name=llamacpp-rpc-pool"
    #    /opt/cluster-infra/nomad/jobs/expert.nomad.j2
    #  loop: "{{ experts }}"
    #  changed_when: true#
