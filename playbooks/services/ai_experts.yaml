- name: Deploy Distributed AI Experts
  hosts: controller_nodes[0] # Run these commands from a single controller node
  connection: local
  gather_facts: yes
  become: no

  vars_files:
    - /home/loki/llama-cluster-upbringing-script/group_vars/all.yaml
    - /home/loki/llama-cluster-upbringing-script/group_vars/models.yaml
    - /home/loki/llama-cluster-upbringing-script/group_vars/external_experts.yaml


  vars:
    experts: "{{ expert_models.keys() | list }}"
    expected_worker_count: "{{ ((groups['workers'] | default([]) | length) if (groups['workers'] | default([]) | length) > 0 else 1) | int }}"
    # This creates a new list containing only the first model object from each expert's list.
    # The template needs the full object (name, filename, memory_mb, etc.), not just the name.
    expert_primary_models: "{{ expert_models.values() | map('first') | list }}"

  tasks:
    - name: Deploy and run the GPU Provider Pool job for each expert
      include_tasks:
        file: "{{ playbook_dir }}/../../ansible/tasks/deploy_expert_gpu_provider.yaml"
      loop: "{{ expert_models.keys() | list }}"
      loop_control:
        loop_var: item

    - name: Read and parse benchmark results
      block:
        - name: Read benchmark log file
          ansible.builtin.slurp:
            src: /var/log/llama_benchmarks.jsonl
          register: benchmark_log_raw

        - name: Parse benchmark JSONL content safely using shell and jq
          ansible.builtin.shell:
            executable: /bin/bash
            cmd: |
              set -o pipefail
              echo '{{ benchmark_log_raw.content }}' | base64 --decode | while IFS= read -r line; do
                # Skip empty or whitespace-only lines
                if [[ -n "${line// /}" ]]; then
                  # Check if the line is valid JSON and print it if it is
                  if echo "$line" | jq -e . >/dev/null 2>&1; then
                    echo "$line"
                  fi
                fi
              done | jq -s .
          register: parsed_json
          changed_when: false
          when: benchmark_log_raw.content is defined

        - name: Set benchmark_results fact from parsed content
          ansible.builtin.set_fact:
            benchmark_results: "{{ (parsed_json.stdout | from_json) if (parsed_json.stdout is defined and parsed_json.stdout) else [] }}"
          when: benchmark_log_raw.content is defined
      rescue:
        - name: Set empty benchmark results on failure
          ansible.builtin.set_fact:
            benchmark_results: []

    - name: Identify successful models from benchmarks
      ansible.builtin.set_fact:
        cacheable: true
        successful_models: "{{ benchmark_results | selectattr('status', 'equalto', 'success') | map(attribute='model') | list }}"

    - name: Identify verified experts by checking their models against benchmark results
      ansible.builtin.set_fact:
        verified_experts: >-
          {{
            verified_experts | default([]) + [item]
          }}
      loop: "{{ experts }}"
      vars:
        # Get the list of model filenames for the current expert
        expert_model_files: "{{ expert_models[item] | map(attribute='filename') | list }}"
        # Check if any of the expert's models are in the list of successful models
        is_verified: "{{ expert_model_files | intersect(successful_models) | length > 0 }}"
      when: is_verified





    - name: Initialize expert_benchmark_data as an empty dictionary
      ansible.builtin.set_fact:
        expert_benchmark_data: {}

    - name: Populate expert_benchmark_data with default values for all experts
      ansible.builtin.set_fact:
        expert_benchmark_data: "{{ expert_benchmark_data | combine({item: {'benchmark_data': [], 'avg_tokens': 0.0}}) }}"
      loop: "{{ experts }}"
      loop_control:
        loop_var: item

    - name: Calculate benchmark data and average tokens per second for each expert
      ansible.builtin.set_fact:
        expert_benchmark_data: >-
          {{
            expert_benchmark_data | combine({
              item: {
                'benchmark_data': (
                  (benchmark_results |
                  selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                  list) if (benchmark_results is defined and benchmark_results | length > 0) else []
                ),
                'avg_tokens': (
                  (
                    (benchmark_results |
                    selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                    map(attribute='tokens_per_second') | list | sum) /
                    (
                      (benchmark_results |
                      selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                      list) | length
                    )
                  ) if (
                    benchmark_results is defined and benchmark_results | length > 0 and
                    (benchmark_results |
                    selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                    list) | length
                  ) > 0 else 0
                )
              }
            })
          }}
      loop: "{{ experts }}"
      loop_control:
        loop_var: item
      when: benchmark_results is defined and benchmark_results | length > 0

    - name: Create Expert Orchestrator job files
      include_tasks:
        file: "{{ playbook_dir }}/../../ansible/tasks/create_expert_job.yaml"
      loop: "{{ experts }}"
      loop_control:
        loop_var: current_expert

    - name: Deploy the Expert Orchestrator services
      ansible.builtin.command: >
        nomad job run
        -var="job_name=expert-{{ item }}"
        -var="service_name=expert-api-{{ item }}"
        -var="rpc_pool_job_name=llamacpp-rpc-pool"
        /tmp/expert-{{ item }}.nomad
      loop: "{{ experts }}"
      changed_when: true
