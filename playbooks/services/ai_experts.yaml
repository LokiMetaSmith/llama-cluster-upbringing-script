- name: Deploy Distributed AI Experts
  hosts: controller_nodes[0] # Run these commands from a single controller node
  connection: local
  gather_facts: yes
  become: no

  vars_files:
    - "../../group_vars/models.yaml"
    - "../../group_vars/external_experts.yaml"

  tags:
    - ai-experts

  vars:
    experts: "{{ expert_models.keys() | reject('equalto', 'router') | list }}"
    expected_worker_count: "{{ ((groups['workers'] | default([]) | length) if (groups['workers'] | default([]) | length) > 0 else 1) | int }}"
    # Extract all models from all experts, then deduplicate based on filename.
    # 1. Flatten the list of lists of models
    # 2. Use 'unique' filter on the objects (requires hashable objects, dicts might work if identical)
    # Better: grouping by filename
    all_models: "{{ expert_models.values() | flatten }}"
    unique_models: "{{ all_models | unique(attribute='filename') }}"
    pipecat_deployment_style: "docker"

  tasks:
    - name: Check for benchmark log file
      stat:
        path: /var/log/llama_benchmarks.jsonl
      register: benchmark_log_stat
      tags:
        - benchmark

    - name: Read and parse benchmark results
      block:
        - name: Read benchmark log file
          ansible.builtin.slurp:
            src: /var/log/llama_benchmarks.jsonl
          register: benchmark_log_raw

        - name: Parse benchmark JSONL content safely using shell and jq
          ansible.builtin.shell:
            executable: /bin/bash
            cmd: |
              set -o pipefail
              echo '{{ benchmark_log_raw.content }}' | base64 --decode | while IFS= read -r line; do
                # Skip empty or whitespace-only lines
                if [[ -n "${line// /}" ]]; then
                  # Check if the line is valid JSON and print it if it is
                  if echo "$line" | jq -e . >/dev/null 2>&1; then
                    echo "$line"
                  fi
                fi
              done | jq -s .
          register: parsed_json
          changed_when: false
          when: benchmark_log_raw.content is defined

        - name: Set benchmark_results fact from parsed content
          ansible.builtin.set_fact:
            benchmark_results: "{{ (parsed_json.stdout | from_json) if (parsed_json.stdout is defined and parsed_json.stdout) else [] }}"
          when: benchmark_log_raw.content is defined
      rescue:
        - name: Set empty benchmark results on failure
          ansible.builtin.set_fact:
            benchmark_results: []
      when: benchmark_log_stat.stat.exists
      tags:
        - benchmark

    - name: Identify successful models from benchmarks
      ansible.builtin.set_fact:
        cacheable: true
        successful_models: "{{ benchmark_results | selectattr('status', 'equalto', 'success') | map(attribute='model') | list }}"
      when: benchmark_results is defined
      tags:
        - benchmark

    - name: Set deployment candidates based on verification
      ansible.builtin.set_fact:
        deployment_candidates: "{{ (unique_models | selectattr('filename', 'in', successful_models) | list) if (successful_models is defined and successful_models | length > 0) else unique_models }}"
      tags:
        - benchmark
        - deploy-gpu-provider

    - name: Ensure pipecatapp Docker image is built
      ansible.builtin.include_tasks:
        file: "{{ playbook_dir }}/../../ansible/tasks/build_pipecatapp_image.yaml"
      tags:
        - deploy-gpu-provider
        - deploy-expert

    - name: Verify pipecatapp Docker image exists
      ansible.builtin.command: docker image inspect pipecatapp:local
      changed_when: false
      register: pipecat_image_info
      tags:
        - deploy-expert

    - name: Set pipecat_image_id fact
      ansible.builtin.set_fact:
        pipecat_image_id: "pipecatapp:local"
      tags:
        - deploy-expert

    - name: Deploy and run the GPU Provider Pool job for verified models
      include_tasks:
        file: "{{ playbook_dir }}/../../ansible/tasks/deploy_model_gpu_provider.yaml"
      loop: "{{ deployment_candidates }}"
      loop_control:
        loop_var: model
      tags:
        - deploy-gpu-provider

    - name: Check for Consul management token file
      ansible.builtin.stat:
        path: /etc/consul.d/management_token
      register: management_token_file
      become: yes
      when: consul_bootstrap_token is not defined or consul_bootstrap_token == ''
      tags:
        - healthcheck

    - name: Read Consul management token
      ansible.builtin.slurp:
        src: /etc/consul.d/management_token
      register: management_token_content
      become: yes
      when:
        - consul_bootstrap_token is not defined or consul_bootstrap_token == ''
        - management_token_file.stat.exists
      tags:
        - healthcheck

    - name: Set consul_bootstrap_token fact
      ansible.builtin.set_fact:
        consul_bootstrap_token: "{{ management_token_content.content | b64decode | trim }}"
      when:
        - consul_bootstrap_token is not defined or consul_bootstrap_token == ''
        - management_token_file.stat.exists
        - management_token_content.content is defined
      tags:
        - healthcheck

    - name: Identify verified experts by checking their models against benchmark results
      ansible.builtin.set_fact:
        verified_experts: >-
          {{
            verified_experts | default([]) + [item]
          }}
      loop: "{{ experts }}"
      vars:
        # Get the list of model filenames for the current expert
        expert_model_files: "{{ expert_models[item] | map(attribute='filename') | list }}"
        # Check if any of the expert's models are in the list of successful models
        is_verified: "{{ expert_model_files | intersect(successful_models) | length > 0 }}"
      when: is_verified and benchmark_results is defined
      tags:
        - benchmark

    - name: Initialize expert_benchmark_data as an empty dictionary
      ansible.builtin.set_fact:
        expert_benchmark_data: {}
      tags:
        - benchmark

    - name: Populate expert_benchmark_data with default values for all experts
      ansible.builtin.set_fact:
        expert_benchmark_data: "{{ expert_benchmark_data | combine({item: {'benchmark_data': [], 'avg_tokens': 0.0}}) }}"
      loop: "{{ experts }}"
      loop_control:
        loop_var: item
      tags:
        - benchmark

    - name: Calculate benchmark data and average tokens per second for each expert
      ansible.builtin.set_fact:
        expert_benchmark_data: >-
          {{
            expert_benchmark_data | combine({
              item: {
                'benchmark_data': (
                  (benchmark_results |
                  selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                  list) if (benchmark_results is defined and benchmark_results | length > 0) else []
                ),
                'avg_tokens': (
                  (
                    (benchmark_results |
                    selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                    map(attribute='tokens_per_second') | list | sum) /
                    (
                      (benchmark_results |
                      selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                      list) | length
                    )
                  ) if (
                    benchmark_results is defined and benchmark_results | length > 0 and
                    (benchmark_results |
                    selectattr('model', 'in', expert_models[item] | map(attribute='filename') | list) |
                    list) | length
                  ) > 0 else 0
                )
              }
            })
          }}
      loop: "{{ experts }}"
      loop_control:
        loop_var: item
      when: benchmark_results is defined and benchmark_results | length > 0 and benchmark_results is defined
      tags:
        - benchmark


    - name: Create Expert Orchestrator job files
      include_tasks:
        file: "{{ playbook_dir }}/../../ansible/tasks/create_expert_job.yaml"
      loop: "{{ experts }}"
      loop_control:
        loop_var: current_expert
      when: benchmark_results is defined
      tags:
        - deploy-expert

    - name: Deploy the Expert Orchestrator services with debugging
      include_tasks:
        file: "{{ playbook_dir }}/../../ansible/tasks/deploy_expert_wrapper.yaml"
      loop: "{{ experts }}"
      loop_control:
        loop_var: expert_name
      when: benchmark_results is defined
      tags:
        - deploy-expert

    - name: Wait for the expert services to be healthy in Consul
      ansible.builtin.uri:
        url: "http://127.0.0.1:8500/v1/health/service/expert-api-{{ item }}"
        headers:
          X-Consul-Token: "{{ consul_bootstrap_token | default(omit) }}"
        return_content: yes
      register: expert_health
      until: >
        expert_health.json is defined and
        expert_health.json | map(attribute='Checks') | flatten | selectattr('Status', 'equalto', 'passing') | list | length > 0
      retries: 60
      delay: 10
      loop: "{{ experts }}"
      changed_when: false
      when: benchmark_results is defined
      tags:
        - healthcheck
