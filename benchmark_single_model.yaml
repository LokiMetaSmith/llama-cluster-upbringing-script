- name: "Run llama-bench for {{ model_item.filename }}"
  ansible.builtin.command:
    cmd: "llama-bench -m /opt/nomad/models/llm/{{ model_item.filename }} -p 32 -n 32"
  register: benchmark_result
  ignore_errors: true
  changed_when: false
  become: yes
  become_user: "{{ target_user }}"
- name: "Append benchmark result for {{ model_item.filename }}"
  ansible.builtin.lineinfile:
    path: /var/log/llama_benchmarks.jsonl
    create: yes
    line: >-
      {{
        {
          'model': model_item.filename,
          'status': 'success' if benchmark_result.rc == 0 else 'fail',
          'tokens_per_second': (
            benchmark_result.stdout
            | regex_findall('total tokens/s.*= *([0-9]+\\.[0-9]+)')
            | first
            | default('0')
            | float
          ),
          'timestamp': ansible_date_time.iso8601,
          'rc': benchmark_result.rc,
          'stdout': (benchmark_result.stdout | regex_replace('\\s+', ' ') | truncate(400)),
          'stderr': (benchmark_result.stderr | regex_replace('\\s+', ' ') | truncate(400))
        } | to_json
      }}
  become: yes
